{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiying surnames\n",
    "with muti-class logistic regression and bag of letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_dataset():\n",
    "    ! wget https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_train.csv.gz \n",
    "    ! wget https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_test.csv.gz \n",
    "    ! mkdir -p data\n",
    "    ! gunzip names_train.csv.gz \n",
    "    ! gunzip names_test.csv.gz\n",
    "    ! mv names*.csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-30 11:00:21--  https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_train.csv.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50237 (49K) [application/octet-stream]\n",
      "Saving to: ‘names_train.csv.gz’\n",
      "\n",
      "names_train.csv.gz  100%[===================>]  49.06K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2021-03-30 11:00:21 (4.71 MB/s) - ‘names_train.csv.gz’ saved [50237/50237]\n",
      "\n",
      "--2021-03-30 11:00:21--  https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_test.csv.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 27541 (27K) [application/octet-stream]\n",
      "Saving to: ‘names_test.csv.gz’\n",
      "\n",
      "names_test.csv.gz   100%[===================>]  26.90K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-03-30 11:00:21 (21.7 MB/s) - ‘names_test.csv.gz’ saved [27541/27541]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unpack_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/names_train.csv'), PosixPath('data/names_test.csv')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = Path(\"data\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data\n",
    "Here we split every last name into letters and assign every letter an id. We represent a last name by a vector of letter frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH/\"names_train.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv(PATH/\"names_test.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adsit</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ajdrna</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Antonowitsch</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Antonowitz</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ballalatak</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0      1\n",
       "0         Adsit  Czech\n",
       "1        Ajdrna  Czech\n",
       "2  Antonowitsch  Czech\n",
       "3    Antonowitz  Czech\n",
       "4    Ballalatak  Czech"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-e720cbe841f1>:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  vocab = sorted(list(set(np.concatenate(np.array(letters)))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' ', \"'\", ',', 'A', 'B', 'C', 'D', 'E', 'F', 'G']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## vocab is a list of unique letters\n",
    "letters = [list(l) for l in df[0].values]\n",
    "vocab = sorted(list(set(np.concatenate(np.array(letters)))))\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " \"'\",\n",
       " ',',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vocab2id is a dictionary mapping letters to a unique number\n",
    "vocab2id = {key:i for i, key in enumerate(vocab)}\n",
    "#vocab2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arabic': 0,\n",
       " 'Chinese': 1,\n",
       " 'Czech': 2,\n",
       " 'Dutch': 3,\n",
       " 'English': 4,\n",
       " 'French': 5,\n",
       " 'German': 6,\n",
       " 'Greek': 7,\n",
       " 'Irish': 8,\n",
       " 'Italian': 9,\n",
       " 'Japanese': 10,\n",
       " 'Korean': 11,\n",
       " 'Polish': 12,\n",
       " 'Portuguese': 13,\n",
       " 'Russian': 14,\n",
       " 'Scottish': 15,\n",
       " 'Spanish': 16,\n",
       " 'Vietnamese': 17}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## label2id is a dictionary mapping classes to ids\n",
    "labels = sorted(df[1].unique())\n",
    "label2id = {key:i for i, key in enumerate(labels)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_letters = len(vocab)\n",
    "num_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df, vocab2id, label2id, num_letters):\n",
    "    \"\"\" Returns encoded data\n",
    "    \n",
    "    outputs:\n",
    "    data: a np array of shape (df.shape[0], num_letters)\n",
    "          data[i, j] counts the number of times letter vocab[j]\n",
    "          is on observation j\n",
    "    y: np array of len df.shape[0]. Id of the labels of each observation.\n",
    "    \"\"\"\n",
    "    data = np.zeros((df.shape[0], num_letters))\n",
    "    y = np.zeros(df.shape[0])\n",
    "    for i, row in df.iterrows():\n",
    "        y[i] = label2id[row[1]]\n",
    "        for c in list(row[0]):\n",
    "            data[i][vocab2id[c]] +=1\n",
    "    return data, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13374, 55), (6700, 55))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train = encode_data(df, vocab2id, label2id, num_letters)\n",
    "x_valid, y_valid = encode_data(val, vocab2id, label2id, num_letters)\n",
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'd', 'i', 's', 't']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking\n",
    "[vocab[i] for i, v in enumerate(x_train[0]) if v==1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'a', 'd', 'j', 'n', 'r']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[i] for i, v in enumerate(x_train[1]) if v==1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "We are going to write a multiclass logistic regression model. Here are the equations:\n",
    "\n",
    "\\begin{align}\n",
    "z_1 & = a_{11}x_1 + \\dots a_{1D}x_D + b_1\\\\\n",
    "z_2 & = a_{21}x_1 + \\dots a_{2D}x_D + b_2 \\\\\n",
    "& \\dots \\\\\n",
    "z_K & = a_{K1}x_1 + \\dots a_{KD}x_D + b_K\n",
    "\\end{align}\n",
    "\n",
    "$$\\hat{y}_k = \\frac{e^{z_k}}{ \\sum_{i=1}^K e^{z_i}}$$\n",
    "\n",
    "\n",
    "Here the observations are $D$ dimensional vectors $x = (x_1, \\dots, x_D)$.\n",
    "\n",
    "In order to get multiclass logistic regression, we do a linear transformation and then a softmax transformation.\n",
    "\n",
    "For numerical reasons, it is better not to apply the softmax directly after the linear transformation but to apply it together with the loss function. The loss function `F.cross_entropy` combines log_softmax and nll_loss in a single function. Therefore to write the model just do the linear transformation with the appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MultiLogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim,output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## CODE HERE\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loss.item()` to get a Python number from a tensor containing a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(model, x_train, y_train, x_valid, y_valid, epochs, lr=0.01, wd=1e-4):\n",
    "    ## get an optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    ## convert your training data to pytorch tensors\n",
    "    x = torch.FloatTensor(x_train)\n",
    "    y = torch.LongTensor(y_train)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        ## evaluate your training data to get y_hat\n",
    "        y_hat = model(x)\n",
    "        ## compute your loss\n",
    "        loss = F.cross_entropy(y_hat,y)\n",
    "        ## zero_grad\n",
    "        optimizer.zero_grad()\n",
    "        ## compute gradients\n",
    "        loss.backward()\n",
    "        ## call gradient descent\n",
    "        optimizer.step()\n",
    "        ## call valid_metrics(model, x_valid, y_valid)\n",
    "        ## print train loss, valid loss and potentially valid accuracy\n",
    "        val_loss, val_acc = valid_metrics(model, x_valid, y_valid)\n",
    "        if i%10 == 1:\n",
    "            print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % \n",
    "                  (loss.item(), val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_metrics(model, x_valid, y_valid):\n",
    "    model.eval()\n",
    "    x = torch.FloatTensor(x_train)\n",
    "    y = torch.LongTensor(y_train)\n",
    "    y_out = model(x)\n",
    "    loss = F.cross_entropy(y_out,y)\n",
    "    _,y_hat = torch.max(y_out,1)\n",
    "    val_acc = y_hat.eq(y).sum().float()/ y.size(0)\n",
    "    \n",
    "    return loss.item(), float(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13374"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.LongTensor(y_train)\n",
    "\n",
    "\n",
    "y.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 2.769 val loss 2.674 and val accuracy 0.235\n",
      "train loss 2.008 val loss 1.955 and val accuracy 0.501\n",
      "train loss 1.644 val loss 1.625 and val accuracy 0.514\n",
      "train loss 1.511 val loss 1.502 and val accuracy 0.521\n",
      "train loss 1.428 val loss 1.421 and val accuracy 0.568\n",
      "train loss 1.367 val loss 1.362 and val accuracy 0.594\n",
      "train loss 1.319 val loss 1.315 and val accuracy 0.603\n",
      "train loss 1.280 val loss 1.277 and val accuracy 0.614\n",
      "train loss 1.249 val loss 1.246 and val accuracy 0.620\n",
      "train loss 1.222 val loss 1.220 and val accuracy 0.631\n",
      "train loss 1.199 val loss 1.197 and val accuracy 0.635\n",
      "train loss 1.180 val loss 1.178 and val accuracy 0.643\n",
      "train loss 1.162 val loss 1.161 and val accuracy 0.647\n",
      "train loss 1.147 val loss 1.146 and val accuracy 0.650\n",
      "train loss 1.133 val loss 1.132 and val accuracy 0.653\n",
      "train loss 1.121 val loss 1.120 and val accuracy 0.656\n",
      "train loss 1.110 val loss 1.109 and val accuracy 0.659\n",
      "train loss 1.101 val loss 1.100 and val accuracy 0.662\n",
      "train loss 1.092 val loss 1.091 and val accuracy 0.663\n",
      "train loss 1.083 val loss 1.083 and val accuracy 0.664\n",
      "train loss 1.076 val loss 1.075 and val accuracy 0.666\n",
      "train loss 1.069 val loss 1.068 and val accuracy 0.668\n",
      "train loss 1.063 val loss 1.062 and val accuracy 0.669\n",
      "train loss 1.057 val loss 1.056 and val accuracy 0.672\n",
      "train loss 1.051 val loss 1.051 and val accuracy 0.673\n",
      "train loss 1.046 val loss 1.046 and val accuracy 0.673\n",
      "train loss 1.041 val loss 1.041 and val accuracy 0.675\n",
      "train loss 1.037 val loss 1.037 and val accuracy 0.675\n",
      "train loss 1.033 val loss 1.032 and val accuracy 0.676\n",
      "train loss 1.029 val loss 1.029 and val accuracy 0.677\n",
      "train loss 1.025 val loss 1.025 and val accuracy 0.677\n",
      "train loss 1.022 val loss 1.021 and val accuracy 0.677\n",
      "train loss 1.019 val loss 1.018 and val accuracy 0.678\n",
      "train loss 1.015 val loss 1.015 and val accuracy 0.679\n",
      "train loss 1.013 val loss 1.012 and val accuracy 0.680\n",
      "train loss 1.010 val loss 1.010 and val accuracy 0.680\n",
      "train loss 1.007 val loss 1.007 and val accuracy 0.680\n",
      "train loss 1.005 val loss 1.004 and val accuracy 0.680\n",
      "train loss 1.002 val loss 1.002 and val accuracy 0.680\n",
      "train loss 1.000 val loss 1.000 and val accuracy 0.680\n",
      "train loss 0.998 val loss 0.998 and val accuracy 0.681\n",
      "train loss 0.996 val loss 0.996 and val accuracy 0.682\n",
      "train loss 0.994 val loss 0.994 and val accuracy 0.681\n",
      "train loss 0.992 val loss 0.992 and val accuracy 0.682\n",
      "train loss 0.990 val loss 0.990 and val accuracy 0.682\n",
      "train loss 0.989 val loss 0.989 and val accuracy 0.682\n",
      "train loss 0.987 val loss 0.987 and val accuracy 0.682\n",
      "train loss 0.986 val loss 0.985 and val accuracy 0.683\n",
      "train loss 0.984 val loss 0.984 and val accuracy 0.682\n",
      "train loss 0.983 val loss 0.983 and val accuracy 0.682\n",
      "train loss 0.981 val loss 0.981 and val accuracy 0.683\n",
      "train loss 0.980 val loss 0.980 and val accuracy 0.683\n",
      "train loss 0.979 val loss 0.979 and val accuracy 0.683\n",
      "train loss 0.978 val loss 0.977 and val accuracy 0.683\n",
      "train loss 0.976 val loss 0.976 and val accuracy 0.683\n",
      "train loss 0.975 val loss 0.975 and val accuracy 0.684\n",
      "train loss 0.974 val loss 0.974 and val accuracy 0.684\n",
      "train loss 0.973 val loss 0.973 and val accuracy 0.684\n",
      "train loss 0.972 val loss 0.972 and val accuracy 0.684\n",
      "train loss 0.971 val loss 0.971 and val accuracy 0.685\n",
      "train loss 0.970 val loss 0.970 and val accuracy 0.685\n",
      "train loss 0.969 val loss 0.969 and val accuracy 0.686\n",
      "train loss 0.969 val loss 0.968 and val accuracy 0.686\n",
      "train loss 0.968 val loss 0.968 and val accuracy 0.686\n",
      "train loss 0.967 val loss 0.967 and val accuracy 0.686\n",
      "train loss 0.966 val loss 0.966 and val accuracy 0.686\n",
      "train loss 0.965 val loss 0.965 and val accuracy 0.687\n",
      "train loss 0.965 val loss 0.965 and val accuracy 0.687\n",
      "train loss 0.964 val loss 0.964 and val accuracy 0.687\n",
      "train loss 0.963 val loss 0.963 and val accuracy 0.688\n",
      "train loss 0.963 val loss 0.963 and val accuracy 0.688\n",
      "train loss 0.962 val loss 0.962 and val accuracy 0.688\n",
      "train loss 0.961 val loss 0.961 and val accuracy 0.689\n",
      "train loss 0.961 val loss 0.961 and val accuracy 0.689\n",
      "train loss 0.960 val loss 0.960 and val accuracy 0.689\n",
      "train loss 0.960 val loss 0.960 and val accuracy 0.689\n",
      "train loss 0.959 val loss 0.959 and val accuracy 0.690\n",
      "train loss 0.958 val loss 0.958 and val accuracy 0.690\n",
      "train loss 0.958 val loss 0.958 and val accuracy 0.690\n",
      "train loss 0.957 val loss 0.957 and val accuracy 0.690\n",
      "train loss 0.957 val loss 0.957 and val accuracy 0.690\n",
      "train loss 0.957 val loss 0.956 and val accuracy 0.690\n",
      "train loss 0.956 val loss 0.956 and val accuracy 0.691\n",
      "train loss 0.956 val loss 0.956 and val accuracy 0.691\n",
      "train loss 0.955 val loss 0.955 and val accuracy 0.691\n",
      "train loss 0.955 val loss 0.955 and val accuracy 0.691\n",
      "train loss 0.954 val loss 0.954 and val accuracy 0.691\n",
      "train loss 0.954 val loss 0.954 and val accuracy 0.691\n",
      "train loss 0.954 val loss 0.953 and val accuracy 0.691\n",
      "train loss 0.953 val loss 0.953 and val accuracy 0.691\n",
      "train loss 0.953 val loss 0.953 and val accuracy 0.691\n",
      "train loss 0.952 val loss 0.952 and val accuracy 0.691\n",
      "train loss 0.952 val loss 0.952 and val accuracy 0.691\n",
      "train loss 0.952 val loss 0.952 and val accuracy 0.691\n",
      "train loss 0.951 val loss 0.951 and val accuracy 0.691\n",
      "train loss 0.951 val loss 0.951 and val accuracy 0.691\n",
      "train loss 0.951 val loss 0.951 and val accuracy 0.691\n",
      "train loss 0.950 val loss 0.950 and val accuracy 0.691\n",
      "train loss 0.950 val loss 0.950 and val accuracy 0.691\n",
      "train loss 0.950 val loss 0.950 and val accuracy 0.691\n"
     ]
    }
   ],
   "source": [
    "model = MultiLogisticRegression(55,18)\n",
    "train_epochs(model, x_train, y_train, x_valid, y_valid, 1000, lr=0.01, wd=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usf",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
