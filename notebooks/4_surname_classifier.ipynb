{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiying surnames\n",
    "with muti-class logistic regression and bag of letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_dataset():\n",
    "    ! wget https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_train.csv.gz \n",
    "    ! wget https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_test.csv.gz \n",
    "    ! mkdir -p data\n",
    "    ! gunzip names_train.csv.gz \n",
    "    ! gunzip names_test.csv.gz\n",
    "    ! mv names*.csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-30 11:00:21--  https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_train.csv.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50237 (49K) [application/octet-stream]\n",
      "Saving to: ‘names_train.csv.gz’\n",
      "\n",
      "names_train.csv.gz  100%[===================>]  49.06K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2021-03-30 11:00:21 (4.71 MB/s) - ‘names_train.csv.gz’ saved [50237/50237]\n",
      "\n",
      "--2021-03-30 11:00:21--  https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_test.csv.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 27541 (27K) [application/octet-stream]\n",
      "Saving to: ‘names_test.csv.gz’\n",
      "\n",
      "names_test.csv.gz   100%[===================>]  26.90K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-03-30 11:00:21 (21.7 MB/s) - ‘names_test.csv.gz’ saved [27541/27541]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unpack_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/names_train.csv'), PosixPath('data/names_test.csv')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = Path(\"data\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data\n",
    "Here we split every last name into letters and assign every letter an id. We represent a last name by a vector of letter frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH/\"names_train.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv(PATH/\"names_test.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adsit</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ajdrna</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Antonowitsch</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Antonowitz</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ballalatak</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0      1\n",
       "0         Adsit  Czech\n",
       "1        Ajdrna  Czech\n",
       "2  Antonowitsch  Czech\n",
       "3    Antonowitz  Czech\n",
       "4    Ballalatak  Czech"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-e720cbe841f1>:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  vocab = sorted(list(set(np.concatenate(np.array(letters)))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' ', \"'\", ',', 'A', 'B', 'C', 'D', 'E', 'F', 'G']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## vocab is a list of unique letters\n",
    "letters = [list(l) for l in df[0].values]\n",
    "vocab = sorted(list(set(np.concatenate(np.array(letters)))))\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " \"'\",\n",
       " ',',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vocab2id is a dictionary mapping letters to a unique number\n",
    "vocab2id = {key:i for i, key in enumerate(vocab)}\n",
    "#vocab2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arabic': 0,\n",
       " 'Chinese': 1,\n",
       " 'Czech': 2,\n",
       " 'Dutch': 3,\n",
       " 'English': 4,\n",
       " 'French': 5,\n",
       " 'German': 6,\n",
       " 'Greek': 7,\n",
       " 'Irish': 8,\n",
       " 'Italian': 9,\n",
       " 'Japanese': 10,\n",
       " 'Korean': 11,\n",
       " 'Polish': 12,\n",
       " 'Portuguese': 13,\n",
       " 'Russian': 14,\n",
       " 'Scottish': 15,\n",
       " 'Spanish': 16,\n",
       " 'Vietnamese': 17}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## label2id is a dictionary mapping classes to ids\n",
    "labels = sorted(df[1].unique())\n",
    "label2id = {key:i for i, key in enumerate(labels)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_letters = len(vocab)\n",
    "num_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df, vocab2id, label2id, num_letters):\n",
    "    \"\"\" Returns encoded data\n",
    "    \n",
    "    outputs:\n",
    "    data: a np array of shape (df.shape[0], num_letters)\n",
    "          data[i, j] counts the number of times letter vocab[j]\n",
    "          is on observation j\n",
    "    y: np array of len df.shape[0]. Id of the labels of each observation.\n",
    "    \"\"\"\n",
    "    data = np.zeros((df.shape[0], num_letters))\n",
    "    y = np.zeros(df.shape[0])\n",
    "    for i, row in df.iterrows():\n",
    "        y[i] = label2id[row[1]]\n",
    "        for c in list(row[0]):\n",
    "            data[i][vocab2id[c]] +=1\n",
    "    return data, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13374, 55), (6700, 55))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train = encode_data(df, vocab2id, label2id, num_letters)\n",
    "x_valid, y_valid = encode_data(val, vocab2id, label2id, num_letters)\n",
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'd', 'i', 's', 't']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking\n",
    "[vocab[i] for i, v in enumerate(x_train[0]) if v==1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'a', 'd', 'j', 'n', 'r']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[i] for i, v in enumerate(x_train[1]) if v==1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "We are going to write a multiclass logistic regression model. Here are the equations:\n",
    "\n",
    "\\begin{align}\n",
    "z_1 & = a_{11}x_1 + \\dots a_{1D}x_D + b_1\\\\\n",
    "z_2 & = a_{21}x_1 + \\dots a_{2D}x_D + b_2 \\\\\n",
    "& \\dots \\\\\n",
    "z_K & = a_{K1}x_1 + \\dots a_{KD}x_D + b_K\n",
    "\\end{align}\n",
    "\n",
    "$$\\hat{y}_k = \\frac{e^{z_k}}{ \\sum_{i=1}^K e^{z_i}}$$\n",
    "\n",
    "\n",
    "Here the observations are $D$ dimensional vectors $x = (x_1, \\dots, x_D)$.\n",
    "\n",
    "In order to get multiclass logistic regression, we do a linear transformation and then a softmax transformation.\n",
    "\n",
    "For numerical reasons, it is better not to apply the softmax directly after the linear transformation but to apply it together with the loss function. The loss function `F.cross_entropy` combines log_softmax and nll_loss in a single function. Therefore to write the model just do the linear transformation with the appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MultiLogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim,output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## CODE HERE\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loss.item()` to get a Python number from a tensor containing a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(model, x_train, y_train, x_valid, y_valid, epochs, lr=0.01, wd=1e-4):\n",
    "    ## get an optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    ## convert your training data to pytorch tensors\n",
    "    x = torch.FloatTensor(x_train)\n",
    "    y = torch.LongTensor(y_train)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        ## evaluate your training data to get y_hat\n",
    "        y_hat = model(x)\n",
    "        ## compute your loss\n",
    "        loss = F.cross_entropy(y_hat,y)\n",
    "        ## zero_grad\n",
    "        optimizer.zero_grad()\n",
    "        ## compute gradients\n",
    "        loss.backward()\n",
    "        ## call gradient descent\n",
    "        optimizer.step()\n",
    "        ## call valid_metrics(model, x_valid, y_valid)\n",
    "        ## print train loss, valid loss and potentially valid accuracy\n",
    "        val_loss, val_acc = valid_metrics(model, x_valid, y_valid)\n",
    "        if i%10 == 1:\n",
    "            print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % \n",
    "                  (loss.item(), val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_metrics(model, x_valid, y_valid):\n",
    "    model.eval()\n",
    "    x = torch.FloatTensor(x_train)\n",
    "    y = torch.LongTensor(y_train)\n",
    "    y_out = model(x)\n",
    "    loss = F.cross_entropy(y_out,y)\n",
    "    _,y_hat = torch.max(y_out,1)\n",
    "    val_acc = y_hat.eq(y).sum().float()/ y.size(0)\n",
    "    \n",
    "    return loss.item(), float(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13374"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.LongTensor(y_train)\n",
    "\n",
    "\n",
    "y.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLogisticRegression(55,18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 2.107 val loss 1.710 and val accuracy 0.509\n",
      "train loss 1.308 val loss 1.265 and val accuracy 0.638\n",
      "train loss 1.118 val loss 1.108 and val accuracy 0.657\n",
      "train loss 1.038 val loss 1.035 and val accuracy 0.672\n",
      "train loss 1.005 val loss 1.002 and val accuracy 0.681\n",
      "train loss 0.984 val loss 0.982 and val accuracy 0.686\n",
      "train loss 0.972 val loss 0.971 and val accuracy 0.685\n",
      "train loss 0.965 val loss 0.964 and val accuracy 0.687\n",
      "train loss 0.959 val loss 0.959 and val accuracy 0.688\n",
      "train loss 0.955 val loss 0.955 and val accuracy 0.690\n",
      "train loss 0.952 val loss 0.952 and val accuracy 0.691\n",
      "train loss 0.950 val loss 0.949 and val accuracy 0.691\n",
      "train loss 0.947 val loss 0.947 and val accuracy 0.691\n",
      "train loss 0.946 val loss 0.946 and val accuracy 0.691\n",
      "train loss 0.944 val loss 0.944 and val accuracy 0.692\n",
      "train loss 0.943 val loss 0.943 and val accuracy 0.692\n",
      "train loss 0.942 val loss 0.942 and val accuracy 0.692\n",
      "train loss 0.941 val loss 0.941 and val accuracy 0.692\n",
      "train loss 0.940 val loss 0.940 and val accuracy 0.692\n",
      "train loss 0.939 val loss 0.939 and val accuracy 0.692\n",
      "train loss 0.939 val loss 0.939 and val accuracy 0.692\n",
      "train loss 0.938 val loss 0.938 and val accuracy 0.692\n",
      "train loss 0.938 val loss 0.938 and val accuracy 0.692\n",
      "train loss 0.937 val loss 0.937 and val accuracy 0.692\n",
      "train loss 0.937 val loss 0.937 and val accuracy 0.692\n",
      "train loss 0.936 val loss 0.936 and val accuracy 0.692\n",
      "train loss 0.936 val loss 0.936 and val accuracy 0.693\n",
      "train loss 0.936 val loss 0.936 and val accuracy 0.693\n",
      "train loss 0.935 val loss 0.935 and val accuracy 0.694\n",
      "train loss 0.935 val loss 0.935 and val accuracy 0.693\n"
     ]
    }
   ],
   "source": [
    "train_epochs(model, x_train, y_train, x_valid, y_valid, 300, lr=0.1, wd=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usf",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
